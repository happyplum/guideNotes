import{_ as n,c as e,o as t,ae as a}from"./chunks/framework.QHFhIKkZ.js";const h=JSON.parse('{"title":"minimind","description":"","frontmatter":{},"headers":[],"relativePath":"ai/minimind/index.md","filePath":"ai/minimind/index.md"}'),p={name:"ai/minimind/index.md"};function r(m,i,o,d,s,_){return t(),e("div",null,[...i[0]||(i[0]=[a('<h1 id="minimind" tabindex="-1">minimind <a class="header-anchor" href="#minimind" aria-label="Permalink to &quot;minimind&quot;">​</a></h1><p><a href="https://github.com/jingyaogong/minimind.git" target="_blank" rel="noreferrer">https://github.com/jingyaogong/minimind.git</a></p><p>conda create -n minimind python=3.12 conda activate minimind</p><p>cd /d D:\\ai\\minimind</p><p>pip install -r requirements.txt -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noreferrer">https://pypi.tuna.tsinghua.edu.cn/simple</a></p><p>安装了vsocde build 2022和rust</p><p>看到LMConfig.py中存在flash_attn配置，还是安装了下flash_attn</p><p>需要修改下LMConfig，默认走的是minimind-zero的配置去的，max_seq_len，dim都要修改为512</p><p>执行“python train_pretrain.py”进行预训练，使用的是pretrain_hq.jsonl进行的训练，3080大概执行了1个小时</p><p>执行“python train_full_sft.py”进行监督微调，默认使用的是sft_mini_512.jsonl进行训练，3080大概执行了3个小时</p><p>执行“python eval_model.py --model_mode 1”测试，发现生成的是minimind-zero，效果一般</p><p>尝试进行修改配置LMconfig.py 查看了参考模型参数版本表，发现有些参数和配置不是完全对应 如果要使用moe训练，需要开启use_moe d_model对应的参数应该是dim share+route=1+4我觉得应该是n_shared_experts和n_routed_experts</p><p>开启moe后重新尝试使用sft_mini_512进行训练 执行“python train_full_sft.py”</p>',13)])])}const c=n(p,[["render",r]]);export{h as __pageData,c as default};
