import{_ as t,c as a,o as n,ae as r}from"./chunks/framework.DtxlzkAV.js";const m=JSON.parse('{"title":"docker","description":"","frontmatter":{},"headers":[],"relativePath":"ai/failed/README.md","filePath":"ai/failed/README.md"}'),i={name:"ai/failed/README.md"};function o(l,e,c,p,s,u){return n(),a("div",null,[...e[0]||(e[0]=[r('<h1 id="docker" tabindex="-1">docker <a class="header-anchor" href="#docker" aria-label="Permalink to &quot;docker&quot;">​</a></h1><p>docker run --runtime nvidia --gpus all --ipc=host --network=host --name qwen2.5 -it qwenllm/qwenvl:2.5-cu121 bash vllm serve Qwen/Qwen2.5-VL-3B-Instruct --port 8000 --host 0.0.0.0 --dtype bfloat16 --limit-mm-per-prompt image=5,video=5 vllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000 --host 0.0.0.0 --dtype bfloat16 --limit-mm-per-prompt image=5,video=5 #不行，运行不起来 docker run --runtime nvidia --gpus=all --ipc=host --network=host --name qwen2.5 -it qwenllm/qwenvl:2.5-cu121 bash</p><h1 id="抱脸提供" tabindex="-1">抱脸提供 <a class="header-anchor" href="#抱脸提供" aria-label="Permalink to &quot;抱脸提供&quot;">​</a></h1><p>docker run --runtime nvidia --gpus all <br> --name my_vllm_container <br> -v ~/.cache/huggingface:/root/.cache/huggingface <br> --env &quot;HUGGING_FACE_HUB_TOKEN=xxx&quot; <br> -p 8000:8000 <br> --ipc=host <br> vllm/vllm-openai:latest <br> --model Qwen/Qwen2.5-VL-3B-Instruct</p><p>docker run --runtime nvidia --gpus all --name my_vllm_container -v ~/.cache/huggingface:/root/.cache/huggingface --env &quot;HUGGING_FACE_HUB_TOKEN=xxx&quot; -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model Qwen/Qwen2.5-VL-3B-Instruct</p><p><a href="https://raw.githubusercontent.com/QwenLM/Qwen2.5-VL/refs/heads/main/docker/Dockerfile-2.5-cu121" target="_blank" rel="noreferrer">https://raw.githubusercontent.com/QwenLM/Qwen2.5-VL/refs/heads/main/docker/Dockerfile-2.5-cu121</a></p><p>docker build -f .\\Dockerfile-2.5-cu121 -t qwen/qwen2.5-vl .. #失败了，windows下build阶段没办法用GPU，导致build vllm的时候内存爆炸</p><h1 id="尝试下载nvidia镜像-手动运行构建" tabindex="-1">尝试下载nvidia镜像，手动运行构建 <a class="header-anchor" href="#尝试下载nvidia镜像-手动运行构建" aria-label="Permalink to &quot;尝试下载nvidia镜像，手动运行构建&quot;">​</a></h1><p>nvidia/cuda:12.1.0-cudnn8-devel-ubuntu20.04</p><p>docker run -it --runtime nvidia --gpus all --ipc=host --network=host nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04 /bin/bash</p><p>git+https://github.com/huggingface/transformers.git</p><p>export MAX_JOBS=8 export NVCC_THREADS=1 export TORCH_CUDA_ARCH_LIST=&quot;7.0 7.5 8.0 8.6 8.9 9.0+PTX&quot; export VLLM_FA_CMAKE_GPU_ARCHES=&quot;80-real;90-real&quot;</p><p><a href="https://github.com/QwenLM/Qwen2.5-VL.git" target="_blank" rel="noreferrer">https://github.com/QwenLM/Qwen2.5-VL.git</a></p><p>pip install git+https://github.com/huggingface/transformers@f3f6c86582611976e72be054675e2bf0abb5f775 pip install accelerate pip install qwen-vl-utils pip install &#39;vllm&gt;=0.7.2&#39;</p>',14)])])}const h=t(i,[["render",o]]);export{m as __pageData,h as default};
